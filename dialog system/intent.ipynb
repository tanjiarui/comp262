{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intent Detection Using CNN & RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wz0jLNgTnMwF"
   },
   "source": [
    "In this notebook we demonstrate various CNN and RNN architectures for the task of intent detection on the ATIS dataset. The ATIS dataset is a standard benchmark dataset for the tast of intent detection. ATIS Stands for Airline Travel Information System. The dataset can we found in the `Data2` folder under the `Data` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T02:51:26.312446Z",
     "start_time": "2020-01-21T02:51:15.878759Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "7GnbW58Yi_7P",
    "outputId": "67fd2a56-575c-4f98-9628-aae205443f8a"
   },
   "outputs": [],
   "source": [
    "#general imports\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "random.seed(0) #for reproducability of results\n",
    "\n",
    "#basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#NN imports\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
    "from keras.models import Model, Sequential\n",
    "from keras.initializers import Constant\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LGFhMpnjo4L1"
   },
   "source": [
    " \n",
    "### Load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_data(filename):\n",
    "    df = pd.read_csv(filename,delim_whitespace=True,names=['word','label'])\n",
    "    beg_indices = list(df[df['word'] == 'BOS'].index)+[df.shape[0]]\n",
    "    sents,labels,intents = [],[],[]\n",
    "    for i in range(len(beg_indices[:-1])):\n",
    "        sents.append(df[beg_indices[i]+1:beg_indices[i+1]-1]['word'].values)\n",
    "        labels.append(df[beg_indices[i]+1:beg_indices[i+1]-1]['label'].values)\n",
    "        intents.append(df.loc[beg_indices[i+1]-1]['label'])    \n",
    "    return np.array(sents, dtype=object), np.array(labels, dtype=object), np.array(intents, dtype=object)\n",
    "\n",
    "def get_data2(filename):\n",
    "    with open(filename) as f:\n",
    "        contents = f.read()\n",
    "    sents,labels,intents = [],[],[]\n",
    "    for line in contents.strip().split('\\n'):\n",
    "        words,labs = [i.split(' ') for i in line.split('\\t')]\n",
    "        sents.append(words[1:-1])\n",
    "        labels.append(labs[1:-1])\n",
    "        intents.append(labs[-1])\n",
    "    return np.array(sents, dtype=object), np.array(labels, dtype=object), np.array(intents, dtype=object)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:16:45.725380Z",
     "start_time": "2020-01-21T03:16:45.669373Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "pl5A0Kwji_7h",
    "outputId": "e7f3b18e-0d39-4428-957b-4e247f3c192b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences : 4455\n",
      "Number of unique intents : 17\n",
      "('i want to fly from baltimore to dallas round trip', 'atis_flight')\n",
      "('round trip fares from baltimore to philadelphia less than 1000 dollars round trip fares from denver to philadelphia less than 1000 dollars round trip fares from pittsburgh to philadelphia less than 1000 dollars', 'atis_airfare')\n",
      "('show me the flights arriving on baltimore on june fourteenth', 'atis_flight')\n",
      "('what are the flights which depart from san francisco fly to washington via indianapolis and arrive by 9 pm', 'atis_flight')\n",
      "('which airlines fly from boston to washington dc via other cities', 'atis_airline')\n",
      "<class 'list'>\n",
      "4455\n",
      "23\n",
      "30\n",
      "23\n",
      "4478\n"
     ]
    }
   ],
   "source": [
    " \n",
    "\n",
    "sents,labels,intents = get_data2('C:/Courses_cent/COMP 262/data_week6/atis-2.train.w-intent.iob')\n",
    "\n",
    "train_sentences = [\" \".join(i) for i in sents]\n",
    "\n",
    "train_texts = train_sentences\n",
    "train_labels= intents.tolist()\n",
    "#Removing any intents with #\n",
    "vals = []\n",
    "\n",
    "for i in range(len(train_labels)):\n",
    "    if \"#\" in train_labels[i]:\n",
    "        vals.append(i)\n",
    "        \n",
    "for i in vals[::-1]:\n",
    "    train_labels.pop(i)\n",
    "    train_texts.pop(i)\n",
    "\n",
    "print (\"Number of training sentences :\",len(train_texts))\n",
    "print (\"Number of unique intents :\",len(set(train_labels)))\n",
    "\n",
    "for i in zip(train_texts[:5], train_labels[:5]):\n",
    "    print(i)\n",
    "    \n",
    "print(type(train_texts))\n",
    "print(len(train_labels))\n",
    "print(len(vals))\n",
    "print(vals[0])\n",
    "print(len(vals))\n",
    "print(len(sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qckNEPKRo8_V"
   },
   "source": [
    "### Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:16:53.184193Z",
     "start_time": "2020-01-21T03:16:52.523898Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "zQynEfVmi_7m",
    "outputId": "38ecc729-f2de-4500-e594-ad4d073dacdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of testing sentences : 497\n",
      "Number of unique intents : 14\n",
      "('i want to fly from boston at 838 am and arrive in denver at 1110 in the morning', 'atis_flight')\n",
      "('show me all round trip flights between houston and las vegas', 'atis_flight')\n",
      "('i would like some information on a flight from denver to san francisco on united airlines', 'atis_flight')\n",
      "('what are the coach flights between dallas and baltimore leaving august tenth and returning august twelve', 'atis_flight')\n",
      "(\"i'm flying from boston to the bay area\", 'atis_flight')\n",
      "2\n",
      "{'atis_flight#atis_airfare', 'atis_airfare#atis_flight_time'}\n"
     ]
    }
   ],
   "source": [
    "##Mayy notice different maipulations\n",
    "#Keep only labels of train in test\n",
    "sents,labels,intents = get_data('C:/Courses_cent/COMP 262/data_week6/atis-2.dev.w-intent.iob')\n",
    "\n",
    "test_sentences = [\" \".join(i) for i in sents]\n",
    "\n",
    "test_texts = test_sentences\n",
    "test_labels = intents.tolist()\n",
    "\n",
    "new_labels = set(test_labels) - set(train_labels)\n",
    "\n",
    "vals = []\n",
    "\n",
    "for i in range(len(test_labels)):\n",
    "    if \"#\" in test_labels[i]:\n",
    "        vals.append(i)\n",
    "    elif test_labels[i] in new_labels:\n",
    "        print(test_labels[i])\n",
    "        vals.append(i)\n",
    "        \n",
    "for i in vals[::-1]:\n",
    "    test_labels.pop(i)\n",
    "    test_texts.pop(i)\n",
    "\n",
    "print (\"Number of testing sentences :\",len(test_texts))\n",
    "print (\"Number of unique intents :\",len(set(test_labels)))\n",
    "\n",
    "for i in zip(test_texts[:5], test_labels[:5]):\n",
    "    print(i)\n",
    "#Mayy    \n",
    "print(len(new_labels))\n",
    "print(new_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nUZsI3ZmpBA2"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 300\n",
    "MAX_NUM_WORDS = 20000 \n",
    "EMBEDDING_DIM = 100 \n",
    "VALIDATION_SPLIT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:16:58.708150Z",
     "start_time": "2020-01-21T03:16:58.445025Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "sk1EXJANi_7p",
    "outputId": "a89ff82e-7cda-4fa3-84c2-eb38679ff881"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 866 unique tokens.\n",
      "[18, 68, 1, 36, 2, 22, 1, 21, 53, 47]\n",
      "[18, 68, 1, 36, 2, 9, 67, 430, 87, 16, 74, 15, 12, 67, 854, 15, 4, 37]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts) #Converting text to a vector of word indexes\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "#Mayy\n",
    "print(train_sequences[0])\n",
    "print(test_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:17:08.464993Z",
     "start_time": "2020-01-21T03:17:08.455586Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "PyTlYCLQi_76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "#Mayy encode the labels\n",
    "le = LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "train_labels = le.transform(train_labels)\n",
    "test_labels = le.transform(test_labels)\n",
    "#Mayy\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:18:09.843555Z",
     "start_time": "2020-01-21T03:18:09.802336Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "XPdDoAAli_8K",
    "outputId": "ee82e0a6-39a1-4893-b9ea-a45db8f30236"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the train data into train and valid is done\n"
     ]
    }
   ],
   "source": [
    "#Converting this to sequences to be fed into neural network. Max seq. len is 1000 as set earlier\n",
    " #initial padding of 0s, until vector is of size MAX_SEQUENCE_LENGTH\n",
    "trainvalid_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "trainvalid_labels = to_categorical(train_labels)\n",
    "\n",
    "test_labels = to_categorical(np.asarray(test_labels), num_classes= trainvalid_labels.shape[1])\n",
    "\n",
    "# split the training data into a training set and a validation set\n",
    "indices = np.arange(trainvalid_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "trainvalid_data = trainvalid_data[indices]\n",
    "trainvalid_labels = trainvalid_labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * trainvalid_data.shape[0])\n",
    "x_train = trainvalid_data[:-num_validation_samples]\n",
    "y_train = trainvalid_labels[:-num_validation_samples]\n",
    "x_val = trainvalid_data[-num_validation_samples:]\n",
    "y_val = trainvalid_labels[-num_validation_samples:]\n",
    "#This is the data we will use for CNN and RNN training\n",
    "print('Splitting the train data into train and valid is done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "### Embedding Matrix\n",
    "We need to prepare our embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:18:41.097840Z",
     "start_time": "2020-01-21T03:18:14.824841Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "gSKw8NHxi_8O",
    "outputId": "fb7a5b97-883c-473f-e016-be984bf4f194"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "Found 400000 word vectors in Glove embeddings.\n",
      "Preparing of embedding matrix is done\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "# Download GloVe 6B from here: https://nlp.stanford.edu/projects/glove/\n",
    "#BASE_DIR = 'Data'\n",
    "#GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "GLOVE_DIR = 'C:/Courses_cent/COMP 262/practical-nlp-master/Ch4/Glove6B/'\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors in Glove embeddings.' % len(embeddings_index))\n",
    "#print(embeddings_index[\"google\"])\n",
    "\n",
    "# prepare embedding matrix - rows are the words from word_index, columns are the embeddings of that word from glove.\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load these pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "print(\"Preparing of embedding matrix is done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN with Pre-Trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:19:56.592225Z",
     "start_time": "2020-01-21T03:19:49.795954Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "w1sdtUNYi_8T",
    "outputId": "bc393907-c69f-4371-8214-ad85f716dbc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Define a 1D CNN model.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 100)          86700     \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 296, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 59, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 55, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 11, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 7, 128)            82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 17)                2193      \n",
      "=================================================================\n",
      "Total params: 333,629\n",
      "Trainable params: 246,929\n",
      "Non-trainable params: 86,700\n",
      "_________________________________________________________________\n",
      "25/25 [==============================] - 3s 112ms/step - loss: 1.1646 - acc: 0.7172 - val_loss: 0.9041 - val_acc: 0.7335\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0061 - acc: 0.7183\n",
      "Test accuracy with CNN: 0.7183098793029785\n"
     ]
    }
   ],
   "source": [
    "print('Define a 1D CNN model.')\n",
    "\n",
    "cnnmodel = Sequential()\n",
    "cnnmodel.add(embedding_layer)\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(MaxPooling1D(5))\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(MaxPooling1D(5))\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(GlobalMaxPooling1D())\n",
    "cnnmodel.add(Dense(128, activation='relu'))\n",
    "cnnmodel.add(Dense(len(trainvalid_labels[0]), activation='softmax'))\n",
    "\n",
    "cnnmodel.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "cnnmodel.summary()\n",
    "\n",
    "#Train the model. Tune to validation set. \n",
    "cnnmodel.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=1, validation_data=(x_val, y_val))\n",
    "#Evaluate on test set:\n",
    "score, acc = cnnmodel.evaluate(test_data, test_labels)\n",
    "print('Test accuracy with CNN:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-Embedding Layer\n",
    "Here, we train a CNN model with an embedding layer which is being trained on the fly instead of using the pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:20:48.874312Z",
     "start_time": "2020-01-21T03:20:39.637994Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ekMBb2Ski_8j",
    "outputId": "c6f2c28d-6e5f-4d69-85b7-b708d9c8475c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining and training a CNN model, training embedding layer on the fly instead of using pre-trained embeddings\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 17)                2193      \n",
      "=================================================================\n",
      "Total params: 2,824,849\n",
      "Trainable params: 2,824,849\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "25/25 [==============================] - 4s 151ms/step - loss: 1.2516 - acc: 0.7179 - val_loss: 0.9198 - val_acc: 0.7335\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0600 - acc: 0.7183\n",
      "Test accuracy with CNN: 0.7183098793029785\n"
     ]
    }
   ],
   "source": [
    "print(\"Defining and training a CNN model, training embedding layer on the fly instead of using pre-trained embeddings\")\n",
    "cnnmodel = Sequential()\n",
    "cnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(MaxPooling1D(5))\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(MaxPooling1D(5))\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(GlobalMaxPooling1D())\n",
    "cnnmodel.add(Dense(128, activation='relu'))\n",
    "cnnmodel.add(Dense(len(trainvalid_labels[0]), activation='softmax'))\n",
    "\n",
    "cnnmodel.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "cnnmodel.summary()\n",
    "\n",
    "#Train the model. Tune to validation set. \n",
    "cnnmodel.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=1, validation_data=(x_val, y_val))\n",
    "#Evaluate on test set:\n",
    "score, acc = cnnmodel.evaluate(test_data, test_labels)\n",
    "print('Test accuracy with CNN:', acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN-Embedding Layer\n",
    "Here, we train a RNN model with an embedding layer which is being trained on the fly instead of using the pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:23:06.806061Z",
     "start_time": "2020-01-21T03:21:34.339598Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "lE5578-ei_8m",
    "outputId": "2d245c0d-fbdf-4986-c29e-133ce5ab732d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining and training an LSTM model, training embedding layer on the fly\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 17)                2193      \n",
      "=================================================================\n",
      "Total params: 2,693,777\n",
      "Trainable params: 2,693,777\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Training the RNN\n",
      "Epoch 1/2\n",
      "98/98 [==============================] - 51s 525ms/step - loss: 0.1634 - accuracy: 0.7336 - val_loss: 0.1019 - val_accuracy: 0.7335\n",
      "Epoch 2/2\n",
      "98/98 [==============================] - 53s 543ms/step - loss: 0.0986 - accuracy: 0.7467 - val_loss: 0.0984 - val_accuracy: 0.7335\n",
      "16/16 [==============================] - 1s 44ms/step - loss: 0.1053 - accuracy: 0.7183\n",
      "Test accuracy with RNN: 0.7183098793029785\n"
     ]
    }
   ],
   "source": [
    "print(\"Defining and training an LSTM model, training embedding layer on the fly\")\n",
    "\n",
    "#modified from: \n",
    "\n",
    "rnnmodel = Sequential()\n",
    "rnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n",
    "rnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "rnnmodel.add(Dense(len(trainvalid_labels[0]), activation='sigmoid'))\n",
    "rnnmodel.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "rnnmodel.summary()\n",
    "\n",
    "print('Training the RNN')\n",
    "rnnmodel.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=2,\n",
    "          validation_data=(x_val, y_val))\n",
    "score, acc = rnnmodel.evaluate(test_data, test_labels,\n",
    "                            batch_size=32)\n",
    "print('Test accuracy with RNN:', acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with Pre-Trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:25:33.571529Z",
     "start_time": "2020-01-21T03:24:04.513325Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "tTTJqKFHi_8x",
    "outputId": "d5514ea5-26d4-4c9d-de30-abf6d3adb8a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining and training an LSTM model, using pre-trained embedding layer\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 100)          86700     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 17)                2193      \n",
      "=================================================================\n",
      "Total params: 206,141\n",
      "Trainable params: 119,441\n",
      "Non-trainable params: 86,700\n",
      "_________________________________________________________________\n",
      "Training the RNN\n",
      "Epoch 1/3\n",
      "98/98 [==============================] - 46s 469ms/step - loss: 0.1506 - accuracy: 0.7169 - val_loss: 0.0969 - val_accuracy: 0.7335\n",
      "Epoch 2/3\n",
      "98/98 [==============================] - 47s 479ms/step - loss: 0.0924 - accuracy: 0.7483 - val_loss: 0.0847 - val_accuracy: 0.7545\n",
      "Epoch 3/3\n",
      "98/98 [==============================] - 49s 502ms/step - loss: 0.0776 - accuracy: 0.7910 - val_loss: 0.0677 - val_accuracy: 0.8181\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 0.0801 - accuracy: 0.7988\n",
      "Test accuracy with RNN: 0.7987927794456482\n"
     ]
    }
   ],
   "source": [
    "print(\"Defining and training an LSTM model, using pre-trained embedding layer\")\n",
    "\n",
    "#modified from: \n",
    "\n",
    "rnnmodel2 = Sequential()\n",
    "rnnmodel2.add(embedding_layer)\n",
    "rnnmodel2.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "rnnmodel2.add(Dense(len(trainvalid_labels[0]), activation='sigmoid'))\n",
    "rnnmodel2.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "rnnmodel2.summary()\n",
    "\n",
    "print('Training the RNN')\n",
    "rnnmodel2.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=3,\n",
    "          validation_data=(x_val, y_val))\n",
    "score, acc = rnnmodel2.evaluate(test_data, test_labels,\n",
    "                            batch_size=32)\n",
    "print('Test accuracy with RNN:', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zTlSMklki_83"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "CNN_RNN_ATIS_intents.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
